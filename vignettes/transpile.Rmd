---
title: "autoparallel-transpile"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{autoparallel-transpile}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Todo: Data analysis workflow where at first you're not sure what you want.

Idea: You could even work totally in the interpreter, and provide a
function such as `save_last_statement()`. This function would look through all
previous correct statements executed through the interpreter and use them
to create the minimal script necessary to reproduce that last statement.
The idea is that as you explore a data set you probably enter many more
statemenets than you actually need, because you're not sure at first what
you're after.

# Transpile

_Transpile_ here refers to translating and changing code to make it more
efficient. Consider this simple script:

```{R}

d = read.csv("data.csv")

hist(d[, 1])

```

This script only uses the first column of `d`, which means that all the
other columns were unnecessary. This optimization only matters when
`data.csv` is sufficiently large.

In general we'd like to do all the preprocessing as early as possible,
saving only what we need.

## Example

Consider the `CargoDescriptions2015.csv` data set. This is a 5.3 GB file
with 5 columns describing goods which arrived in the United States via shipping
containers in 2015.  

### data.table

Suppose we want to count the number of unique
containers. The naive way is to read in the entire data set. Using the high
performance multi-threaded `fread` function from `data.table`.

```{R}

system.time(cargo <- data.table::fread("~/data/CargoDescriptions2015.csv"))

```

This takes 2 minutes, 35 seconds on a 2016 Macbook Pro with 8 cores and
uses 5.5 GB of memory in the R process.

Since we're already using `data.table` lets use that to answer the original
question.

```{R}

system.time(num_container <- uniqueN(cargo[, "container_number"]))

nrow(cargo) / num_container

```

This takes between 9 and 10 seconds. We see that on average each container
appears about 5 times.

Can we do this faster with `data.table` by reading in only a subset of the
columns? To realistically time this we first need to remove this file from
the file cache.

```
$ vmtouch -ve CargoDescriptions2015.csv
Evicting CargoDescriptions2015.csv

           Files: 1
     Directories: 0
   Evicted Pages: 1383045 (5G)
         Elapsed: 0.24117 seconds
```

Now lets see how long `data.table` takes to read just the columns that we
need.

```{R}

# 11-12 seconds for 4 threads
# 30 seconds for 1 thread
system.time(containers <- data.table::fread("~/data/CargoDescriptions2015.csv"
                    , select = "container_number", nThread = 4L))


system.time(num_container <- uniqueN(containers))

```

Selecting the column at the data load step reduces the load time from 2.5
minutes to 12 seconds, which is more than an order of magnitude
improvement. This brings the total program run time from 165 seconds to 22
seconds.

Side note: this requires a full scan of a file that's around 5 GB. It
happened in 12 seconds. The file was not cached in memory. This means my
laptop SSD is able to load data into memory with a rate of at least 5 GB /
12 seconds = 400 MB / second. This sounds reasonable, but can I verify?

The moral of the story is that the exact same calculations
can be computed more efficiently if we know ahead of time what is necessary.


### base R

For comparison, let's measure the same operation in base R. We can select
the columns we need through the `colClasses` parameter to `read.table`.

```{R}

# 607 seconds
system.time(cargo <- read.table("~/data/CargoDescriptions2015.csv"
    , header = TRUE, sep = ",", quote = "\"", comment.char = ""
    , colClasses = c("numeric", "character", "integer", "integer", "character")
    ))

# 493 seconds
system.time(containers <- read.table("~/data/CargoDescriptions2015.csv"
    , header = TRUE, sep = ",", quote = "\"", comment.char = ""
    , colClasses = c("NULL", "character", "NULL", "NULL", "NULL")
    ))

# 2.96 seconds. 3X faster in base R than data.table.
system.time(num_container <- length(unique(containers[, 1])))

```

These read times suggest that perhaps it would be desirable to just transform
regular R code into `data.table` or `iotools` if IO is the bottleneck.


### with shell preprocessing

Another way to potentially make R faster is to use the shell to select the
column of interest.
This approach is less robust than the others because it is unlikely to
handle corner cases well, ie. escape characters and quoted strings
containing the delimiter character.

Running on the above data set I see issues with 
value `"SACRAMENTO,"`, which gets incorrectly cut to
`"SACRAMENTO`.

I'm expecting 43754131 observations.

```{R}

system.time(containers <- read.table(
    pipe("cut -d , -f 2 ~/data/CargoDescriptions2015.csv")
    , header = TRUE, sep = ",", quote = "\"", comment.char = ""
    , colClasses = "character"
))

# Didn't work:
#
#Warning message:
#In scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :
#  EOF within quoted string
#  > dim(containers)
#  [1] 8294588       1
#

system.time(containers2 <- scan(
    pipe("cut -d , -f 2 ~/data/CargoDescriptions2015.csv")
    , what = character(), skip = 1, sep = "\n"
))
# Read 43754131 items
#    user  system elapsed
#    147.888   2.216  97.881

length(containers2)

```

The second command seems to work, with the caveats above. This takes R down
from 493 seconds to 98 seconds, a factor of 5. `data.table` is still a
factor of 10 faster and much more robust.


## Design

If external packages such as `data.table` are being used, can we preserve
these calls? This requires understanding the semantics of the library functions
to use. So we'd have to do it custom for every package that we depend on.
Unless these packages provide drop in replacement for the target
statements.

More generally we could keep the calls that read everything in and then
remove what we don't need immediately after. Call this the "general
approach" This helps with saving memory,
but probably won't help with performance otherwise. In other words, we
might see a difference if we're up against memory limits.

__Possible Use Cases__

- Reducing memory footprint
- Increasing speed
Or should I just pick one?  Only for reading data?  And should I just
depend on and tie myself to data.table?

Ideally I don't make this specific to data.table.

Suppose I take the general approach. In which cases will this help with
memory pressure? The goal is to avoid making copies of a large data frame.
I need to experiment to see exactly when this happens.

The experiment shows that the unused columns in a data frame will not be
copied. For a matrix they should be, but in my experience people are
more likely to use a subset of the columns of a data frame versus a matrix.

In the general case we read in the unused columns and then never copy them. 
The only way this can alleviate memory pressure is if the program meets the
following conditions:

1. Has enough memory to contain the whole object in the first place
2. Subsequently requires more memory than is available

This seems rare to me.

