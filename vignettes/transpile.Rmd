---
title: "autoparallel-transpile"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{autoparallel-transpile}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Todo: Data analysis workflow where at first you're not sure what you want.

Idea: You could even work totally in the interpreter, and provide a
function such as `save_last_statement()`. This function would look through all
previous correct statements executed through the interpreter and use them
to create the minimal script necessary to reproduce that last statement.
The idea is that as you explore a data set you probably enter many more
statemenets than you actually need, because you're not sure at first what
you're after.

Idea: Can we load only columns of a `data.frame` with R's `load` / `save`
commands?

Idea: `remove_nse()` function to replace nonstandard evaluation with
equivalant standard evaluation call. Transforms `subset(mtcars, select = mpg)` into 
`mtcars[, "mpg", drop = FALSE]`

# Transpile

_Transpile_ here refers to translating and changing code to make it more
efficient. Consider this simple script:

```{R}

d = read.csv("data.csv")

hist(d[, 1])

```

This script only uses the first column of `d`, which means that all the
other columns were unnecessary. This optimization only matters when
`data.csv` is sufficiently large.

In general we'd like to do all the preprocessing as early as possible,
saving only what we need.

## Example

Consider the `CargoDescriptions2015.csv` data set. This is a 5.3 GB file
with 5 columns describing goods which arrived in the United States via shipping
containers in 2015.  

### data.table

Suppose we want to count the number of unique
containers. The naive way is to read in the entire data set. Using the high
performance multi-threaded `fread` function from `data.table`.

```{R}

system.time(cargo <- data.table::fread("~/data/CargoDescriptions2015.csv"))

```

This takes 2 minutes, 35 seconds on a 2016 Macbook Pro with 8 cores and
uses 5.5 GB of memory in the R process.

Since we're already using `data.table` lets use that to answer the original
question.

```{R}

system.time(num_container <- uniqueN(cargo[, "container_number"]))

nrow(cargo) / num_container

```

This takes between 9 and 10 seconds. We see that on average each container
appears about 5 times.

Can we do this faster with `data.table` by reading in only a subset of the
columns? To realistically time this we first need to remove this file from
the file cache.

```
$ vmtouch -ve CargoDescriptions2015.csv
Evicting CargoDescriptions2015.csv

           Files: 1
     Directories: 0
   Evicted Pages: 1383045 (5G)
         Elapsed: 0.24117 seconds
```

Now lets see how long `data.table` takes to read just the columns that we
need.

```{R}

# 11-12 seconds for 4 threads
# 30 seconds for 1 thread
system.time(containers <- data.table::fread("~/data/CargoDescriptions2015.csv"
                    , select = "container_number", nThread = 4L))


system.time(num_container <- uniqueN(containers))

```

Selecting the column at the data load step reduces the load time from 2.5
minutes to 12 seconds, which is more than an order of magnitude
improvement. This brings the total program run time from 165 seconds to 22
seconds.

Side note: this requires a full scan of a file that's around 5 GB. It
happened in 12 seconds. The file was not cached in memory. This means my
laptop SSD is able to load data into memory with a rate of at least 5 GB /
12 seconds = 400 MB / second. This sounds reasonable, but can I verify?

Wed Sep 20 15:08:24 PDT 2017

Just came back to this and found that using vmtouch can load this file into
cached memory in 8 seconds, implying a rate of 625 MB / second. So what I
saw previously is indeed reasonable.

The moral of the story is that the exact same calculations
can be computed more efficiently if we know ahead of time what is necessary.


### base R

For comparison, let's measure the same operation in base R. We can select
the columns we need through the `colClasses` parameter to `read.table`.

```{R}

# 607 seconds
system.time(cargo <- read.table("~/data/CargoDescriptions2015.csv"
    , header = TRUE, sep = ",", quote = "\"", comment.char = ""
    , colClasses = c("numeric", "character", "integer", "integer", "character")
    ))

# 493 seconds
system.time(containers <- read.table("~/data/CargoDescriptions2015.csv"
    , header = TRUE, sep = ",", quote = "\"", comment.char = ""
    , colClasses = c("NULL", "character", "NULL", "NULL", "NULL")
    ))

# 2.96 seconds. 3X faster in base R than data.table.
system.time(num_container <- length(unique(containers[, 1])))

```

These read times suggest that perhaps it would be desirable to just transform
regular R code into `data.table` or `iotools` if IO is the bottleneck.


### with shell preprocessing

Another way to potentially make R faster is to use the shell to select the
column of interest.
This approach is less robust than the others because it is unlikely to
handle corner cases well, ie. escape characters and quoted strings
containing the delimiter character.

Running on the above data set I see issues with 
value `"SACRAMENTO,"`, which gets incorrectly cut to
`"SACRAMENTO`.

I'm expecting 43754131 observations.

```{R}

system.time(containers <- read.table(
    pipe("cut -d , -f 2 ~/data/CargoDescriptions2015.csv")
    , header = TRUE, sep = ",", quote = "\"", comment.char = ""
    , colClasses = "character"
))

# Didn't work:
#
#Warning message:
#In scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :
#  EOF within quoted string
#  > dim(containers)
#  [1] 8294588       1
#

system.time(containers2 <- scan(
    pipe("cut -d , -f 2 ~/data/CargoDescriptions2015.csv")
    , what = character(), skip = 1, sep = "\n"
))
# Read 43754131 items
#    user  system elapsed
#    147.888   2.216  97.881

length(containers2)

```

The second command seems to work, with the caveats above. This takes base R down
from 493 seconds to 98 seconds, a factor of 5. It also should use much less
memory because R only ever sees the column that it needs. `data.table` is still 
10 times faster than this and much more robust.

If I'm going to do something like this I need to be able to create these
`cut` commands. It would be better to rely on as few external dependencies as
possible, bash `cut` is not available on Windows. What if I used Python?
That's another external dependency. It wouldn't surprise me if it was as
fast as the cut command. Now I'm curious.

```{R}

system.time(containers2 <- scan(
    pipe("python ~/data/select_column.py ~/data/CargoDescriptions2015.csv 2")
    , what = character(), skip = 1, sep = "\n"
))
# Read 43754131 items
#    user  system elapsed
#    188.837   2.993 138.850

```

Of course introducing a dependency on another programming language is far
from ideal. I wonder if there's a way to pipe the R commands to get
pipeline parallelism?  Judging by the user and elapsed times here we did
get some pipeline parallelism, both for the Bash and Python approaches.

What is the limiting factor here? Preprocessing or other?
I can find best case performance by just reading the file by itself.

```{R}

N = 43754131 

system.time(containers2 <- scan("~/data/container_number.csv"
    , what = character(), skip = 1, sep = "\n", n = N
))

# Read 43754131 items
#    user  system elapsed
#     70.371   0.517  70.899


# scan() seems to have three parameters similar to `n`
system.time(containers2 <- scan("~/data/container_number.csv"
    , what = character(), skip = 1, sep = "\n", n = N, nlines = N
))

# Read 43754131 items
#    user  system elapsed
#     52.205   0.242  52.454
#    user  system elapsed
#     50.457   0.254  50.724
#    user  system elapsed
#     50.439   0.271  50.725

```

The times above are consistent even when I remove the file from cache. This
makes sense based on what I saw above with `data.table`. Then around 50
seconds is a lower bound for how fast this load can happen with `scan`. The
Python and bash `cut` versions are 2 to 3 times slower than this.


## Design

If external packages such as `data.table` are being used, can we preserve
these calls? This requires understanding the semantics of the library functions
to use. So we'd have to do it custom for every package that we depend on.
Unless these packages provide drop in replacement for the target
statements.

More generally we could keep the calls that read everything in and then
remove what we don't need immediately after. Call this the "general
approach" This helps with saving memory,
but probably won't help with performance otherwise. In other words, we
might see a difference if we're up against memory limits.

__Possible Use Cases__

- Reducing memory footprint
- Increasing speed
Or should I just pick one?  Only for reading data?  And should I just
depend on and tie myself to data.table?

Ideally I don't make this specific to data.table.

Suppose I take the general approach. In which cases will this help with
memory pressure? The goal is to avoid making copies of a large data frame.
I need to experiment to see exactly when this happens.

The experiment shows that the unused columns in a data frame will not be
copied. For a matrix they should be, but in my experience people are
more likely to use a subset of the columns of a data frame versus a matrix.

In the general case we read in the unused columns and then never copy them. 
The only way this can alleviate memory pressure is if the program meets the
following conditions:

1. Has enough memory to contain the whole object in the first place
2. Subsequently requires more memory than is available

This seems rare to me.


## Assumptions

Assume that column selections can be inferred by static code analysis. For
example, we can do static analysis on the following:

```{R}
# Literals
mtcars[, 1L]
mtcars[, "mpg"]
mtcars$mpg

# Variables defined from literals. This use case requires
# constant propagation
cols = c("cyl", disp")
mtcars[, cols]
```

While we cannot do static analysis on these:
```{R}

# Computed columns
col = read.csv("something_external.csv")[1, 1]
mtcars[, col]

# Random columns
mtcars[, min(rpois(1, 1) + 1, ncol(mtcars))]
```

So we assume that the column selection is a literal after constant
propagation.


## Implementation

How can we tell which columns are used?

```{R}

library(CodeDepends)

s = getInputs(readScript("columns.R"))

```

This doesn't seem all that helpful.

Fri Sep 22 10:31:23 PDT 2017

For the moment I'm ignoring NSE such as `subset`.

Thinking now that it's fine to depend on `data.table`.
`data.table::fread` has a `select` parameter for column names. It would be
more convenient for our purposes here if `select` took an integer vector of
the column indices instead. Indices are more general because:

- Not every text file has column names
- Not every data frame has meaningful column names
- Column names may not be unique

One approach is to take all the uses of column names and map them into
integers.
The code will go through three representations then:

__Original__ including mixed names and integers:

```{R}

mpg = mtcars$mpg
cyl = mtcars[, 2]
disp = mtcars[, "disp"]
wt = mtcars[[5]]

```

__Name Replacement__ substitutes the names with integers, and converts all
`data.frame` subsetting commands into single `[`. Assume that we know the
column names.

```{R}

mpg = mtcars[, 1]
cyl = mtcars[, 2]
disp = mtcars[, 3]
wt = mtcars[, 5]

```

__Subset mapping__ maps the original indices to corresponding indices in the
smaller `data.frame`. The index map is a sorted integer vector of the
columns that are used.

```{R}

index_map = c(1, 2, 3, 5)

# Assuming fread supports integers here
.mtcars = fread(..., select = index_map)

mpg = .mtcars[, 1]
cyl = .mtcars[, 2]
disp = .mtcars[, 3]
wt = .mtcars[, 4]   # This one changes

```

What are the limitations of the approach that I've just outlined? 

I haven't yet considered subsetting rows, there may be a way to do that
efficiently.  A common way is to subset based on the value of some column.
I could do this by keeping on open file pointer, reading a chunk of the
data, subset it, add that subset to a list, then rbind the subsets
together. This potentially lets R quickly process files larger than memory.

Stepping back, R has many ways to write programs. To simplify tasks here we
first put the code into a canonical form, and then do "surgery" on it.
