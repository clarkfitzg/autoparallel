---
title: "autoparallel-interactive"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{autoparallel-interactive}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Interactive

This vignette discusses the interactive use of this package.


## Basic Example

We'll start with a basic example.

```{r}

library(autoparallel)

x = list(1:10, letters, LETTERS)

do = parallelize(x)
do

```

The parallel evaluator named `do` splits `x` into approximately equal parts
so that one can run code in parallel.

```{r}

lapply(x, head)

do(lapply(x, head))

```

Calling the same code through `do` produces the same result as the base r
case. Under the hood `do` sent the code to different r processes for
evaluation. 

This is meant for interactively building functions and analysis on large
data sets. The interactive feature is sending functions from one's global
workspace to the parallel workers. We can see the results of the improved /
debugged version of the function as we work on them.

```{r}

# An analysis function
myfun = function(x) x[1:2]

do(lapply(x, myfun))

# Oops I actually need the first 4
myfun = function(x) x[1:4]

# Now we see the new results of myfun
do(lapply(x, myfun))

```

## Working with many files

A realistic example is working with many files simultaneously. The US
Veterans Administration (VA) Court appeals are one such example. Each file
contains the summary of an appeal.
One can download a handful from the VA servers as follows:

```{r, echo = FALSE}

setwd("~/data/vets/")

```

```{r download, eval = FALSE}

datadir = "vets_appeals"
dir.create(datadir)

fnames = paste0("1719", 100:266, ".txt")
urls = paste0("https://www.va.gov/vetapp17/files3/", fnames)

Map(download.file, urls, fnames)

```

The file names themselves are small, so we can cheaply distribute them
among the parallel workers.

```{r}

filenames = list.files(datadir, full.names = TRUE)
length(filenames)

do = parallelize(filenames)

```

The following code actually loads the data contained in the files and
assigns the result into `appeals` on the cluster. 

```{r}

do({
    appeals <- lapply(filenames, readLines)
    appeals <- sapply(appeals, paste, collapse = "\n")
    appeals <- enc2utf8(appeals)
    NULL
})

```

The braces along with the final `NULL` are necessary to avoid transferring
the large data set from the workers back to the manager.

The code above only assigned `appeals` to the global environment of the
workers. It does not exist in the manager process.

```{r}

# FALSE
"appeals" %in% ls()

```

The parallel evaluator allows us to compute on it with the same code that
can be used for serial r.

```{r}

do(length(appeals))
do(class(appeals))

```

We may want to look more closely at those cases which have been remanded
for further evidence. If they're a reasonably small subset we may choose to
bring them back into the manager process for further non parallel analysis.
This would be useful to see the warnings that may come from our code, for example.

```{r}

# Check how many we're about to bring back
do(sum(grepl("REMAND", appeals)))

# Bring them back from the workers
remand <- do(appeals[grepl("REMAND", appeals)])

length(remand)

```

In summary, when working with larger data sets it's efficient to minimize
the data movement. We avoided it in this case by only distributing the
relatively small vector of file names and having each worker independently
load the files that it needed, thus keeping the data in place on that
worker.

## Cleaning up

When finished it's a good idea to shut down the cluster.
The cluster is available as an attribute on the parallel evaluator object.

```{r}

parallel::stopCluster(attr(do, "cluster"))

```

## From old intro:

```{R}
library(autoparallel)

# Suppose docs is a large list of (XML) documents
docs = list(letters, letters, LETTERS)

do = parallelize(docs, workers = 2)
```

`parallelize` returns a closure that maintains server state. It evaluates expressions
much like `eval` and `parallel::clusterEvalq`.

```{R}
getfirst = function(doc) doc[1:5]

do(lapply(docs, getfirst))
```

The line above will do the equivalent of `lapply(docs, getfirst)`, but more
efficiently in parallel on the distributed data set. 

To implement later is when `getfirst()` or dependencies of `getfirst` change then
these should be exported to the cluster. First pass is to just export
the one function each time.
