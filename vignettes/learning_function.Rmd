---
title: "autoparallel-learning-functions"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{autoparallel-learning-functions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Learning functions

Tue Aug 29 10:37:03 PDT 2017

Imagine functions that get better as they are used. 
What if functions could adapt themselves to different arguments?

As a simple example, consider statistical computation on an $n \times p$
matrix $X$, ie. we have $n$ $p$ dimensional observations. Suppose we want
to call a function $f(X)$. There may be several possible efficient
implementations of a function $f$. Which is most efficient may depend on the
computer system and the values of $n$ and $p$.


## Current concepts

Learning functions already
exist; any function that caches results or intermediate computations will
be faster when called with the same arguments the second time. A prominent
and well executed example is R's matrix package, which caches matrix
decompositions for subsequent use.

Another example from different languages is JIT compilation. A function
is written in a general way, for example:

```{R}
dotprod = function (x, y)
{
    sum(x * y)
}
```

With JIT compilation when `dotprod()` is first called with both `x, y`
double precision floating point number then it will take time to compile a
version of `dotprod` specialized to these argument types, and then it will
call it on these arguments. When `dotprod()` is subsequently called with other
floating point arguments the same precompiled version will be discovered
and used again.

## Tuning function

`autoparallel` lets us improve functions using `learn_func()`. The simplest
way to use `learn_func()` is to pass multiple implementations as arguments.
Consider the following two implementations of linear regression which
extract the ordinary least squares coefficients.

```{R}

# Standard user facing R implementation
ols = function (data)
{
    fit = lm(y ~ . -1, data = data)
    coef(fit)
}

# Direct implementation of formula
ols_naive = function (data)
{
    ycol = (colnames(data) == "y")
    y = as.matrix(data[, ycol])
    X = as.matrix(data[, !ycol])
    XtXinv = solve(t(X) %*% X)
    XtXinv %*% t(X) %*% y
}

ols_clever = function (data)
{
    ycol = (colnames(data) == "y")
    y = as.matrix(data[, ycol])
    X = as.matrix(data[, !ycol])
    XtX = crossprod(X)
    Xty = crossprod(X, y)
    solve(XtX, Xty)
}

```

The naive direct implementation of the formula is much faster for some small
data sets:

```{R}

library(microbenchmark)

n0 = 100
p0 = 2
X0 = matrix(rnorm(n0 * p0), nrow = n0)
y0 = rnorm(n0)
smalldata = data.frame(X0, y = y0)

microbenchmark(ols(smalldata), ols_naive(smalldata), times = 10)

```

While for larger data sets the naive version is again much faster, totally
wrecking the point I was trying to make. D'oh!

```{R}

n1 = 10000
p1 = 500
X1 = matrix(rnorm(n1 * p1), nrow = n1)
y1 = rnorm(n1)
medium_data = data.frame(X1, y = y1)

beta0 = ols(medium_data)
beta1 = ols_naive(medium_data)
beta2 = ols_clever(medium_data)

max(abs(beta0 - beta1))
max(abs(beta0 - beta2))

microbenchmark(ols_clever(medium_data), ols_naive(medium_data), times = 10)

```
