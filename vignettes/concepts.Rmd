---
title: "makeParallel concepts"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{makeParallel-concepts}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!--
[![Travis-CI Build
Status](https://travis-ci.org/clarkfitzg/makeParallel.svg?branch=master)](https://travis-ci.org/clarkfitzg/makeParallel)
-->

This vignette explains the concepts used by makeParallel. If you would
like to quickly get started then see the vignette titled "makeParallel
Quick Start", `vignette("makeParallel-quickstart")`.

This project is experimental and ambitious. If you would like to write
parallel R today then you would do well to start with an established
package, such as the recommended [parallel
package](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf)
which has been included with R since version 2.14. The CRAN Task View
[High-Performance and Parallel Computing with
R](https://cran.r-project.org/view=HighPerformanceComputing)
provides a thorough review of available third party software. 


# makeParallel

This is a package to take general R code and transform it into
parallel code. General R code means code that just uses functions in base
R, no additional packages.

This differs from most approaches to parallel programming in R. Most
conventional approaches define a certain API that the user then defines
their program around. The problem with this is that then this code becomes
tied to the package, so one has to write code in different ways.

The appeal of this approach is that you don't have to change your code in
any way. By allowing this _system_ to change your code you can benefit from
underlying improvements in the system, and change your code in ways that
you may have never thought of, or that were manually infeasible.


## Technique

Code transformation relies on static code analysis. This means we don't
actually run any code until the user specifically asks for it.
The [CodeDepends package](https://github.com/duncantl/CodeDepends)
currently provides many underlying tools.

As we build more tools that are useful for general purpose static R code
analyis we've been putting them in the [CodeAnalysis
package](https://github.com/duncantl/CodeAnalysis) which this package will
eventually come to depend on.

## Plans

__Use information on data__

We're currently working on extending makeParallel to take into account the
size and nature of the data to be analyzed. For example, if the data won't
fit into memory then this typically a totally different approach to reading
the data and performing the computations.




The main idea of this package is to transform serial R programs written in
base R into parallel programs. In this sense it acts as a
[transpiler](https://en.wikipedia.org/wiki/Source-to-source_compiler). This
automatic program transformation differs from current parallel technologies
which require the user to explicitly write code according to a particular
programming model / API. The broader goal is to bring in more information on the
system and the data to compute on. In this way we build more intelligence
into the system, freeing the user to write higher level code that also
performs better.


The following diagram illustrates the computational model for the steps in inferring task
parallelism as implemented in the function `makeParallel`.

![basic model](basic_model.png)

In this diagram the rectangles represent base classes defined in this package, and
the edges represent generic methods. The diagram depicts the following:

1. The end user supplies some input code, which could be the name of a file
   or an actual expression.
2. `inferGraph` statically analyzes the code and constructs a dependency
   graph between the expressions based primarily on [use-definition
   chains](https://en.wikipedia.org/wiki/Use-define_chain).
3. `schedule` assigns each expression to a time and processor while
   respecting the constraints of the dependency graph.
4. `generate` creates executable code from the schedule.
5. `writeCode` simply writes the code to a file and returns the expression
   containing the code.

These steps are all designed to be modular. The most interesting step to
change is the `scheduler`. In general scheduling a task graph is NP hard,
and many algorithms exist to solve this problem.

## Types of parallelism

By default, `schedule`

Choosing a scheduler. The default scheduler is `data_scheduler`, which
uses the more common data parallelism found in R. The alternative is
`task_scheduler`, which uses task parallelism to generate a schedule.
`data_scheduler` and `task_scheduler` are generic functions. _Why? They
don't dispatch on any args. Ah, but they could do different things
depending on the system. Hmm, could I fit the Hive code generation into
this model? Maybe possible. It would be nice to design it in such a way
that I'm not limited from extending it later, if possible._


Data parallelism executes the same code on different data. R's vectorized
computational model is amenable to data parallelism. In contrast, task
parallelism executes multiple different pieces of code simultaneously. 

The only way to get a speedup from task parallelism is if two or more long
running parts of the program can run simultaneously. This could happen when
reading two large files, for example. If we happen to know how long
each of the expressions will take to run we can pass these times in as an
argument.

```{r echo=FALSE, eval=FALSE, results='hide',message=FALSE}

    n = 1e6

    xfile = tempfile()
    write.csv(data.frame(x = rnorm(n)), xfile, row.names = FALSE)

    yfile = tempfile()
    write.csv(data.frame(y = rnorm(n)), yfile, row.names = FALSE)

```

```{r, fig.width = 8, fig.height = 6, results = "hide"}
library(makeParallel)

code = parse(text = "
    x = read.csv(xfile)
    y = read.csv(yfile)
    xy = sort(c(x[, 1], y[, 1]))
    ")

pcode = makeParallel(code, scheduler = scheduleTaskList
    , exprTime = c(1.25, 1.24, 0.2))

plot(schedule(pcode))
```

This plot illustrates the schedule returned from the scheduling algorithm.
The completion time of the script is the last time a processor is busy on
the graph. Efficient schedules complete earlier. This plot is a useful
diagnostic- if all cores are mostly busy that's a good sign. If only one
core does almost all the work then either 1) the code can't be improved
through task parallelism, or 2) the default scheduling algorithm chose a
poor schedule. In general task scheduling problem is NP hard, but we leave
hooks in for users to supply their own scheduling algorithm.

The generated code is this disgusting thing that no sane person would ever
write by hand. But it does have a few virtues:

- It runs
- It does the same thing as the serial version while being faster
- We never had to explicitly create it.

```{r}
writeCode(pcode)
```



## Customizability

`makeParallel` allows us to pass in functions that customize the behavior.
For example, we may want the generated code to explicitly set the number of
parallel workers to `N` as specified by the option `mc.cores` in the parallel
package. The following function prepends the expression
`options(mc.cores = N)` to the generated code.

```{r}
coresGenerate <- function(schedule, mc.cores = 2L, ...)
{
    # Rely on the method dispatch for the actual work.
    out <- generate(schedule, ...)

    # Construct an expression containing the desired code.
    setCores <- substitute(options(mc.cores = MC_CORES)
                          , list(MC_CORES = mc.cores))

    # Combine the newly constructed expression with what would have been
    # generated otherwise. Passing `file = FALSE` ensures `writeCode`
    # doesn't write to a file.
    out@code <- c(setCores, writeCode(out, file = FALSE))
    out
}
```

We can use this function as follows:

```{r}
lapplyCode <- parse(text = "
    x <- list(a = 1:10, beta = exp(-3:3), logic = c(TRUE,FALSE,FALSE,TRUE))
    m1 <- lapply(x, mean)
")

transformed <- makeParallel(lapplyCode, generator = coresGenerate,
                            generatorArgs = list(mc.cores = 3L))
```

When we extract the code from this object with `writeCode` we see that it
sets `options(mc.cores = 3L)` as the first line.

```{r}
writeCode(transformed)
```

```{r, echo = FALSE, results = "hide"}
# Testing, make sure the docs do what they say!
stopifnot(writeCode(transformed)[[1]] == quote(options(mc.cores = 3L)))
```


## Extensibility

Some schedulers must be tied to their code generators. `inferGraph,
schedule`, and `generate` are all generic functions, so we can allow user
defined classes to extend the system through R's S4 object oriented
programming system. 

Building on the example above, we can define a class `WorkerMapSchedule`
containing `MapSchedule` that adds a slot for `mc.cores`.

```{r}
setClass("WorkerMapSchedule", slots = c(mc.cores = "integer"), contains = "MapSchedule")
```

Here's a helper constructor function:

```{r}
workerMapSchedule = function(graph, mc.cores = 2L, ...)
{
    message(sprintf("User defined scheduler, mc.cores = %s", mc.cores))
    out = mapSchedule(graph, ...)
    new("WorkerMapSchedule", out, mc.cores = mc.cores)
}
```

Now we need to associate a code generator with `WorkerMapSchedule`.
We can use an existing one or we can define our own.
Since we've already defined `coresGenerate` above we'll just wrap that.

```{r}
setMethod("generate", "WorkerMapSchedule", function(schedule, ...)
    coresGenerate(as(schedule, "MapSchedule"), mc.cores = schedule@mc.cores, ...)
)
```

Finally, we can use the code as follows:

```{r}
transformed <- makeParallel(code, scheduler = workerMapSchedule, mc.cores = 3L)

writeCode(transformed)
```

```{r, echo = FALSE, results = "hide"}
# Testing, make sure the docs do what they say!
stopifnot(writeCode(transformed)[[1]] == quote(options(mc.cores = 3L)))
```


In this section we went beyond the basic customization in the previous
section in two ways. First, we extended the existing class hierarchy by
defining our own scheduler. Second, we defined methods and relied on method
dispatch to control some aspects of the code generation process. We did not
have to touch the dependency graph computations.

## Summary

Users can specify the scheduling and code generation steps by passing
functions or defining methods. Here's an associated mental model.

![extensible model](extensible.png)

In summary, `makeParallel` can be used in the following ways:

- `makeParallel(code)` simple, common use case
- `makeParallel("some_script.R")` dispatches on the class of
  the first argument.
- `makeParallel(taskgraph = tg)` skips the setup part of the function in
  case the user has already done that or they have a special task graph to
  use.
- `makeParallel(code, scheduler = my_scheduler, code_generator =
  my_code_generator)` allows users to both customize the steps in the process
    by passing in their own functions to perform them.
- `makeParallel(code, scheduler = fork_join_schedule)` dispatches on the
  class allowing users to extend the system through defining their own
  classes.


## Diagnostics

This package provides a few tools to inspect the generated schedules.


## Road Map

The following features are planned, but not yet implemented.

### Short Term

1. Show which code actually changed, perhaps by providing a `diff()`
   method.
1. Add constraints for processes, ie. code that performs plotting must
   happen in one process
2. More efficient scheduling algorithms and code generation
3. Detection of objects with reference semantics to handle appropriately.

### Longer Term

1. Allow users to pass information about the data, ie. dimensions.
2. Infer dimensions and run time when possible.
3. Generate code that uses systems besides R, for example Apache Hive.
