---
title: "autoparallel-evolve"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{autoparallel-evolve}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Evolving functions

Tue Aug 29 10:37:03 PDT 2017

Imagine functions that get better as they are used. 
What if functions could adapt themselves to different arguments?

As a simple example, consider statistical computation on an $n \times p$
matrix $X$, ie. we have $n$ $p$ dimensional observations. Suppose we want
to call a function $f(X)$. There may be several possible efficient
implementations of a function $f$. Which is most efficient may depend on the
computer system and the values of $n$ and $p$.


## Current concepts

Learning functions already
exist; any function that caches results or intermediate computations will
be faster when called with the same arguments the second time. A prominent
and well executed example is R's matrix package, which caches matrix
decompositions for subsequent use.

Another example from different languages is JIT compilation. A function
is written in a general way, for example:

```{R}
dotprod = function (x, y)
{
    sum(x * y)
}
```

With JIT compilation when `dotprod()` is first called with both `x, y`
double precision floating point number then it will take time to compile a
version of `dotprod` specialized to these argument types, and then it will
call it on these arguments. When `dotprod()` is subsequently called with other
floating point arguments the same precompiled version will be discovered
and used again.

## This implme

`autoparallel` lets us improve functions using `evolve()`. The simplest
way to use `evolve()` is to pass multiple implementations as arguments.
Consider the following two implementations of linear regression which
extract the ordinary least squares coefficients.

```{R}

# Direct implementation of formula
ols_naive = function (X, y)
{
    if(ncol(X) == 2){
        X = X[, 2]
        mX = mean(X)
        my = mean(y)
        Xcentered = X - mX
        b1 = sum(Xcentered * (y - my)) / sum(Xcentered^2)
        b0 = my - b1 * mX
        c(b0, b1)
    } else {
        XtXinv = solve(t(X) %*% X)
        XtXinv %*% t(X) %*% y
    }
}

ols_clever = function (X, y)
{
    XtX = crossprod(X)
    Xty = crossprod(X, y)
    solve(XtX, Xty)
}

```

Before timing we may not be sure which of these implementations are faster.
Then we can pass both implementations into `evolve()` and let it figure
it out for us.

```{R}

library(autoparallel)

ols = evolve(ols_naive, ols_clever)

```

`ols()` is a function with the same signature (or a superset of the
signatures?) of `ols_naive()` and `ols_clever()`. 

## Ideas

- `ols()` will try different implementations. This implies that there must
  be different possible implementations to try.
- `ols()` times itself
- `ols()` detects easily parallelizable parts of code and can change them.



```{R}

library(microbenchmark)

n = 100
p = 1
ones = rep(1, n)
X = matrix(c(ones, rnorm(n * p)), nrow = n)
y = rnorm(n)

beta_naive = ols_naive(X, y)
beta_clever = ols_clever(X, y)

max(abs(beta_naive - beta_clever))

microbenchmark(ols_naive(X, y), ols_clever(X, y), times = 10)

```

With these numbers the naive version is slightly better.


## Related ideas

Is there a way to "merge" functions? In the OLS case for least squares I
give three implementations. Some may work better on special cases. Could we
pull all of that logic into one function? That's somewhat what I'm doing
here.
