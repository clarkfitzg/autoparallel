---
title: "autoparallel"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{autoparallel}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

[![Travis-CI Build
Status](https://travis-ci.org/clarkfitzg/autoparallel.svg?branch=master)](https://travis-ci.org/clarkfitzg/autoparallel)

# autoparallel

_experimental library to transform serial R code into parallel_

If you would like to write parallel R today then you should start with a
well established package. The CRAN Task View: [High-Performance and
Parallel Computing with
R](https://cran.r-project.org/web/views/HighPerformanceComputing.html)
provides a comprehensive review of available software. The [parallel
package](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf),
included as a recommended package with R since R 2.14, provides the core functionality to do
multiprocessing.

## quick start

The simplest thing we can do with this package is directly replace serial
code with parallel versions. The following example comes from the documentation for 
`lapply`:

```{r}
library(autoparallel)

code = parse(text = "
    x <- list(a = 1:10, beta = exp(-3:3), logic = c(TRUE,FALSE,FALSE,TRUE))
    m <- lapply(x, mean)
    ")

pcode = autoparallel(code)
```

Usually we call `autoparallel()` on the name of a file containing an R
script. But we can also call it directly on the code as we do here, which
makes for a self contained example. The resulting object has replaced
`lapply` with `parallel::mclapply`.

```{r}
code

pcode

eval(pcode$output_code)

# same result as evaluating the serial code
m
```

Besides changing R's apply style functions, `data_parallel` also replaces
specific types of `for` loops with parallel versions.


## high level

The main idea of this package is to transform serial R programs written in
base R into parallel programs. In this sense it acts as a
[transpiler](https://en.wikipedia.org/wiki/Source-to-source_compiler). This
automatic program tranformation differs from current parallel technologies
which require the user to explicitly write code for a specified parallel
programming model. The broader goal is to bring in more information on the
system and the data to compute on. In this way we build more intelligence
into the system, freeing the user to write higher level code that also
performs better.

## task parallelism

Choosing a scheduler. The default scheduler is `data_scheduler`, which
uses the more common data parallelism found in R. The alternative is
`task_scheduler`, which uses task parallelism to generate a schedule.
`data_scheduler` and `task_scheduler` are generic functions. _Why? They
don't dispatch on any args. Ah, but they could do different things
depending on the system. Hmm, could I fit the Hive code generation into
this model? Maybe possible. It would be nice to design it in such a way
that I'm not limited from extending it later, if possible._


Data parallelism executes the same code on different data. R's vectorized
computational model is amenable to data parallelism. In contrast, task
parallelism executes multiple different pieces of code simultaneously. 

The only way to get a speedup from task parallelism is if two or more long
running parts of the program can run simultaneously. This could happen when
reading two large files, for example. If we happen to know how long
each of the expressions will take to run we can pass these times in as an
argument.

```{r echo=FALSE, results='hide',message=FALSE}

    n = 1e6

    xfile = tempfile()
    write.csv(data.frame(x = rnorm(n)), xfile, row.names = FALSE)

    yfile = tempfile()
    write.csv(data.frame(y = rnorm(n)), yfile, row.names = FALSE)

```

```{r, fig.width = 6, fig.height = 6}
code = parse(text = "
    x = read.csv(xfile)
    y = read.csv(yfile)
    xy = sort(c(x[, 1], y[, 1]))
    ")

pcode = autoparallel(code, scheduler = task_scheduler
    , expr_times = c(1.25, 1.24, 0.2))

plot(pcode$schedule)
```

This plot illustrates the schedule returned from the scheduling algorithm.
The completion time of the script is the last time a processor is busy on
the graph. Efficient schedules complete earlier. This plot is a useful
diagnostic- if all cores are mostly busy that's a good sign. If only one
core does almost all the work then either 1) the code can't be improved
through task parallelism, or 2) the default scheduling algorithm chose a
poor schedule. In general task scheduling problem is NP hard, but we leave
hooks in for users to supply their own scheduling algorithm.

The generated code is this disgusting thing that no sane person would ever
write by hand:

```{r}
pcode$output_code
```


At the simplest level users write:

```{r, eval = FALSE}
autoparallel("some_script.R")
```

`autoparallel` determines that `some_script.R` is a file on your computer,
runs all of the steps, and by default writes the newly generated code to
`gen_some_script.R`.

This is the basic computational model for the steps in inferring task
parallelism.

![basic model](basic_model.png)

## Extensibility

Some schedulers must be tied to their code generators.
`task_graph, schedule`, and `generate_code` are all generic functions, so
we can extend the system through object oriented programming. For
example, one could define a scheduling algorithm `fork_join_schedule` that returns a more
specialized schedule as well as an associated code generator.

```{r, eval = FALSE}
fork_join_schedule = function(taskgraph, maxworkers = 2L, ...)
{
    # ... more code here ...
    class(result) = c("ForkJoinSchedule", "Schedule")
    result
}

generate_code.ForkJoinSchedule = function(schedule, ...)
{
    # ... more code here ...
}
```

We can call this new code as follows:

```{r, eval = FALSE}
autoparallel(code, scheduler = fork_join_schedule)
```

Users can specify the scheduling and code generation steps by passing
functions or defining methods. Here's an associated mental model.

![extensible model](extensible.png)

In summary, `autoparallel` can be used in the following ways:

- `autoparallel(code)` simple, common use case
- `autoparallel("some_script.R")` methods for different classes of
  the first argument.
- `autoparallel(taskgraph = tg)` skips the setup part of the function in
  case the user has already done that or they have a special task graph to
  use.
- `autoparallel(code, scheduler = my_scheduler, code_generator =
  my_code_generator)` allows users to both customize the steps in the process
    by passing in their own functions to perform them.
- `autoparallel(code, scheduler = fork_join_schedule)` dispatches on the
  class allowing users to extend the system through defining their own
  classes.


## Next Steps

This package needs several more features before it can be used on general
types of code.

1. Add constraints for processes, ie. code that performs plotting must
   happen in one process
2. More efficient scheduling algorithms and code generation
3. Detection of objects with reference semantics to throw appropriate
   errors


## Related Work

Several existing packages provide a more consistent interface to parallel
computation.

Landau's [drake](https://ropensci.github.io/drake/) provides task
parallelism similar to GNU make.

Bengstton's
[futures](https://cran.r-project.org/web/packages/future/index.html)
provides a mechanism for parallel asynchronous evaluation of R code
across different systems.

Bischl and Lang's
[parallelMap](https://cran.r-project.org/package=parallelMap) provides a
parallel version of `Map()` supporting different execution backends
including local, multicore, mpi and BatchJobs. The
[batchtools](https://cran.r-project.org/package=batchtools) package
supports HPC systems.

BÃ¶hringer's
[parallelize.dynamic](https://cran.r-project.org/package=parallelize.dynamic)
provides the `parallelize_call()` function to dynamically parallelize a
single function call.

Wang's [valor](https://github.com/wanghc78/valor) vectorizes `lapply` calls
into single function calls. In some sense this is the most related project,
because the main purpose of valor is to actually transform code.
