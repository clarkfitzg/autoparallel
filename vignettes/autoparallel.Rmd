---
title: "autoparallel"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{autoparallel}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

[![Travis-CI Build
Status](https://travis-ci.org/clarkfitzg/autoparallel.svg?branch=master)](https://travis-ci.org/clarkfitzg/autoparallel)

# autoparallel

_experimental library to transform serial R code into parallel_

If you would like to write parallel R today then you should start with a
well established package. The CRAN Task View: [High-Performance and
Parallel Computing with
R](https://cran.r-project.org/web/views/HighPerformanceComputing.html)
provides a comprehensive review of available software. The [parallel
package](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf),
included as a recommended package with R since R 2.14, provides the core functionality to do
multiprocessing.

## quick start

The simplest thing we can do with this package is directly replace serial
code with parallel versions. The following example comes from the documentation for 
`lapply`:

```{r}
library(autoparallel)

code = parse(text = "
    x <- list(a = 1:10, beta = exp(-3:3), logic = c(TRUE,FALSE,FALSE,TRUE))
    m <- lapply(x, mean)
    ")

pcode = data_parallel(code)
pcode

eval(pcode$output_code)

# same result as evaluating the serial code
m
```

`data_parallel` looks for uses of R's apply family of functions and
specific types of `for` loops and replaces these with parallel versions.


## high level

The main idea is to transform serial R programs written in base R into
parallel programs. This automatic program tranformation differs from
current parallel technologies which require the user to explicitly write
code for a given parallel programming model.
The broader goal is to incorporate more intelligence into the system,
freeing the user to write higher level code that also performs better.

## task parallelism

Data parallelism executes the same code on different data. R's vectorized
computational model is amenable to data parallelism. In contrast, task
parallelism executes multiple different pieces of code simultaneously. 

The only way to get a speedup from task parallelism is if two or more long
running parts of the program can run simultaneously. This could happen when
reading two large files, for example. If we happen to know how long
each of the statements will take we can pass this in also.

```{r echo=FALSE, results='hide',message=FALSE}

    n = 1e6

    xfile = tempfile()
    write.csv(data.frame(x = rnorm(n)), xfile, row.names = FALSE)

    yfile = tempfile()
    write.csv(data.frame(y = rnorm(n)), yfile, row.names = FALSE)

```

```{r}
code = parse(text = "
    x = read.csv(xfile)
    y = read.csv(yfile)
    xy = sort(c(x[, 1], y[, 1]))
    ")

pcode = task_parallel(code, expr_times = c(1.25, 1.24, 0.2))

plot(pcode$schedule)
```


At the simplest level users write:

```{r, eval = FALSE}
task_parallel("some_script.R")
```

`task_parallel` determines that `some_script.R` is a file on your computer,
runs all of the steps, and by default writes the newly generated code to
`gen_some_script.R`.

This is the basic mental model for the steps that autoparallel takes:

![basic model](basic_model.png)

## Extensibility

Some schedulers must be tied to their code generators.
`task_graph, schedule`, and `generate_code` are all generic functions, so
we can extend the system through object oriented programming. For
example, one could define a scheduling algorithm `fork_join_schedule` that returns a more
specialized schedule as well as an associated code generator.

```{r, eval = FALSE}
fork_join_schedule = function(taskgraph, maxworkers = 2L, ...)
{
    # ... more code here ...
    class(result) = c("ForkJoinSchedule", "Schedule")
    result
}

generate_code.ForkJoinSchedule = function(schedule, ...)
{
    # ... more code here ...
}
```

We can call this new code as follows:

```{r, eval = FALSE}
task_parallel(code, scheduler = fork_join_schedule)
```

Users can specify the scheduling and code generation steps by passing
functions or defining methods. Here's an associated mental model.

![extensible model](extensible.png)

In summary, `task_parallel` can be used in the following ways:

- `task_parallel(code)` simple, common use case
- `task_parallel("some_script.R")` methods for different classes of
  the first argument.
- `task_parallel(taskgraph = tg)` skips the setup part of the function in
  case the user has already done that or they have a special task graph to
  use.
- `task_parallel(code, schedule = my_scheduler, code_generator =
  my_code_generator)` allows users to customize the steps in the process
    by passing in their own functions to perform them.
- `task_parallel(code, schedule = fork_join_schedule)` dispatches on the
  class allowing users to extend the system through defining their own
  classes.
